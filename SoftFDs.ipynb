{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed46426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:43.480480Z",
     "start_time": "2021-05-25T16:14:43.461440Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94450e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:00:13.619658Z",
     "start_time": "2021-05-25T16:00:03.448845Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4156fdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:55.738785Z",
     "start_time": "2021-05-25T16:14:46.058505Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession\n",
    "conf = ps.SparkConf().setMaster('local').setAppName(\"softFD\")\n",
    "sc = ps.SparkContext('local[4]', '', conf=conf) # uses 4 cores on your local machine\n",
    "spark=SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e351bb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:59.263343Z",
     "start_time": "2021-05-25T16:14:55.740582Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"csv\").option(\"header\", \"true\").load('./data/2018-01_bme280sof.csv')\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load('./data/ToyTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16dd305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:59.279330Z",
     "start_time": "2021-05-25T16:14:59.265330Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce6a448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:59.550352Z",
     "start_time": "2021-05-25T16:14:59.281296Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.createOrReplaceTempView(\"sofia\")\n",
    "df.createOrReplaceTempView(\"toy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91f5341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:59.613334Z",
     "start_time": "2021-05-25T16:14:59.551332Z"
    }
   },
   "outputs": [],
   "source": [
    "# small_df = spark.sql(\"select * from sofia limit 1000\")\n",
    "toy_df = spark.sql(\"select * from toy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde14198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:14:59.660294Z",
     "start_time": "2021-05-25T16:14:59.615336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I1', 'I2', 'S1', 'S2', 'N1', 'N2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "# schema = small_df.columns\n",
    "schema = toy_df.columns\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f8c063f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:40:16.020551Z",
     "start_time": "2021-05-25T16:40:16.006389Z"
    }
   },
   "outputs": [],
   "source": [
    "LHS = {'I1'}\n",
    "RHS = 'S2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf0742d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:39:18.691579Z",
     "start_time": "2021-05-25T16:39:18.678525Z"
    }
   },
   "outputs": [],
   "source": [
    "zeroVal1 = ([], 0)\n",
    "mergeVal1 = (lambda aggregated, el: (aggregated[0] + [el[0]], aggregated[1] + el[1]))    \n",
    "mergeComb1 = (lambda agg1,agg2:agg1+agg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ecf2b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:39:19.316110Z",
     "start_time": "2021-05-25T16:39:19.309111Z"
    }
   },
   "outputs": [],
   "source": [
    "zeroVal2 = ([], [], [])\n",
    "mergeVal2 = (lambda aggregated, el: (aggregated[0] + [el[0]], aggregated[1] + [el[1]], aggregated[2] + [el[2]]))    \n",
    "mergeComb2 = (lambda agg1,agg2:agg1+agg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf975997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:39:19.910335Z",
     "start_time": "2021-05-25T16:39:19.899336Z"
    }
   },
   "outputs": [],
   "source": [
    "zeroVal3 = ([], [])\n",
    "mergeVal3 = (lambda aggregated, el: (aggregated[0] + [el[0]], aggregated[1] + [el[1]]))    \n",
    "mergeComb3 = (lambda agg1,agg2:agg1+agg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91b47f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:40:19.532112Z",
     "start_time": "2021-05-25T16:40:17.378801Z"
    }
   },
   "outputs": [],
   "source": [
    "# map and reduce into a five-tuple rdd\n",
    "rdd1 = toy_df.rdd.map(lambda x: (*LHS, RHS, tuple(x[idx] for idx in list(map(lambda y: schema.index(y),LHS))), x[schema.index(RHS)]))\\\n",
    "    .map(lambda tpe: (tpe,1)).reduceByKey(add).map(lambda x: ((x[0][:-1]), ((x[0][-1], x[1]), x[1])))\\\n",
    "    .aggregateByKey(zeroVal1,mergeVal1,mergeComb1).map(lambda x: ((x[0]), list(map(lambda r: round(r[1] / x[1][1], 3),x[1][0]))))\\\n",
    "    .map(lambda x: ((x[0]), 1 - reduce(lambda p1, p2:p1+p2,list(map(lambda p: p*(1-p), x[1])))))\\\n",
    "    .map(lambda x: ((x[0][:-1]), (x[0][-1], x[1]))).aggregateByKey(zeroVal3,mergeVal3,mergeComb3).filter(lambda tup: all(t >= 0.8 for t in tup[1][1]))\\\n",
    "    .map(lambda tup: tup[0]).collect()\n",
    "# rdd2 = rdd1.map(lambda x: ((x[0][:-1]), ((x[0][-1], x[1]), x[1]))).aggregateByKey(zeroVal1,mergeVal1,mergeComb1).map(lambda x: ((x[0]), list(map(lambda r: round(r[1] / x[1][1], 3),x[1][0])))).collect()\n",
    "# rdd2 = rdd1.map(lambda x: ((x[0][:-1]), ((x[0][-1], x[1]), x[1]))).aggregateByKey(zeroVal1,mergeVal1,mergeComb1).map(lambda x: ((x[0]), (list(map(lambda r: (r[0], round(r[1] / x[1][1], 3)),x[1][0])), x[1][1]))).collect()\n",
    "# rdd2 = rdd1.map(lambda x: ((x[0][:-1]), ((x[0][-1], x[1]), x[1]))).aggregateByKey(zeroVal1,mergeVal1,mergeComb1).map(lambda x: ((x[0][:-1]), (x[0][-1], *x[1]))).aggregateByKey(zeroVal2,mergeVal2,mergeComb2).collect()\n",
    "for line in rdd1:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad43c5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:15:05.319606Z",
     "start_time": "2021-05-25T16:15:05.297612Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_computational_graph(RHS, schema):\n",
    "    \"\"\"\n",
    "    Output\n",
    "    ----------\n",
    "    A dictionary where\n",
    "    key: level\n",
    "    value: list of current level's candidates, candidates are in the format of set\n",
    "    -----\n",
    "\n",
    "    \"\"\"\n",
    "    computational_graph=dict()\n",
    "    for level in range(3):\n",
    "        #use brute force to generate candidates for each level\n",
    "        computational_graph[level]=[]\n",
    "        if level== 0:\n",
    "            for attribute  in schema:\n",
    "                if attribute !=RHS:\n",
    "                    computational_graph[level].append(set([attribute]))\n",
    "\n",
    "        else:\n",
    "            for element1 in computational_graph[level-1]:\n",
    "                for element2 in computational_graph[0]:\n",
    "                    newelement = element1.union(element2)\n",
    "                    if newelement not in computational_graph[level]:\n",
    "                        if len(newelement)==level+1:\n",
    "                            computational_graph[level].append(newelement)    \n",
    "\n",
    "    return computational_graph\n",
    "def get_candidates(level, computational_graph):\n",
    "    return computational_graph[level]\n",
    "def prune_graph(level,current_level_result,computational_graph):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    -------\n",
    "    current_level_result: (soft/delta) functional dependencies discovered by algorithm, data structure: a list of candidates where candidates are in the format of sets\n",
    "    computational_graph: A dict where key:level value: list of current level's candidates, candidates are in the format of set\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    A pruned computational graph\n",
    "    \"\"\"\n",
    "    # Candidates are pruned because minimal FD are already discovered\n",
    "\n",
    "    # prune candidates after this level by verifying whether the next level has previous level's candidates as subset\n",
    "    new_computational_graph = copy.deepcopy(computational_graph)\n",
    "    while level<2:\n",
    "        level+=1\n",
    "        for LHS in current_level_result:\n",
    "            for candidate in computational_graph[level]:\n",
    "                if LHS.issubset(candidate):\n",
    "                    if candidate in new_computational_graph[level]:\n",
    "                        new_computational_graph[level].remove(candidate)\n",
    "\n",
    "\n",
    "    return new_computational_graph\n",
    "def transform_res(FDs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    FDs: a list of (soft/delta) functional dependencies, where elements are tuples(LHS,RHS), LHS is in the format of set\n",
    "\n",
    "    Output\n",
    "    ---------\n",
    "    current_level_result: a dictionary where key: RHS value: a list of LHS where candidates are in the form of sets\n",
    "    \"\"\"\n",
    "\n",
    "    current_level_result=dict()\n",
    "    for (LHS,RHS) in FDs:\n",
    "        if RHS not in current_level_result.keys():\n",
    "            current_level_result[RHS]=[]\n",
    "\n",
    "        current_level_result[RHS].append(LHS)\n",
    "\n",
    "    return current_level_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e70732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:33:43.933866Z",
     "start_time": "2021-05-25T16:33:43.922871Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_softFDs_pairs(level, df,current_level_candidates):\n",
    "    softFDs=[]\n",
    "    rdds=spark.sparkContext.emptyRDD()\n",
    "    for RHS in current_level_candidates.keys():\n",
    "        for LHS in current_level_candidates[RHS]:\n",
    "            rdds=df.rdd.map(lambda x: (*LHS, RHS, tuple(x[idx] for idx in list(map(lambda y: schema.index(y),LHS))), x[schema.index(RHS)]))\\\n",
    "                .map(lambda tpe: (tpe,1)).reduceByKey(add)\\\n",
    "                .map(lambda x: ((x[0][:-1]), ((x[0][-1], x[1]), x[1])))\\\n",
    "                .aggregateByKey(zeroVal1,mergeVal1,mergeComb1)\\\n",
    "                .map(lambda x: ((x[0]), list(map(lambda r: round(r[1] / x[1][1], 3),x[1][0]))))\\\n",
    "                .map(lambda x: ((x[0]), 1 - reduce(lambda p1, p2:p1+p2,list(map(lambda p: p*(1-p), x[1])))))\\\n",
    "                .map(lambda x: ((x[0][:-1]), (x[0][-1], x[1]))).aggregateByKey(zeroVal3,mergeVal3,mergeComb3)\\\n",
    "                .filter(lambda tup: all(t >= 0.8 for t in tup[1][1]))\\\n",
    "                .map(lambda x:(*x[0][:level],x[0][level])).collect()\n",
    "                \n",
    "            for item in rdds:\n",
    "                softFDs.append(({*item[:-1]},item[-1]))\n",
    "\n",
    "    return softFDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ad88a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:37:38.566983Z",
     "start_time": "2021-05-25T16:37:38.556983Z"
    }
   },
   "outputs": [],
   "source": [
    "computational_graph=dict()\n",
    "for RHS in schema:\n",
    "    computational_graph[RHS]=generate_computational_graph(RHS,schema)\n",
    "\n",
    "#Define current_level_candidates\n",
    "current_level_candidates=dict()\n",
    "\n",
    "for RHS in schema:\n",
    "    current_level_candidates[RHS] = get_candidates(0,computational_graph[RHS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb563161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-25T16:38:51.879346Z",
     "start_time": "2021-05-25T16:37:46.815768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 65.0475845336914 seconds ---\n",
      "[({'S1'}, 'I1'), ({'N1'}, 'I1'), ({'N2'}, 'I1'), ({'I1'}, 'S1'), ({'N1'}, 'S1'), ({'N2'}, 'S1'), ({'N2'}, 'N1')]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "softFDs = find_softFDs_pairs(1, toy_df, current_level_candidates)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(softFDs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
